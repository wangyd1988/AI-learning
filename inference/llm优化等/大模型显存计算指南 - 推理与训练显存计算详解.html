<!DOCTYPE html>
<html lang="zh-CN">
 <head>
  <meta charset="utf-8"/>
  <link href="https://blog.csdn.net/qq_41797451/article/details/144516971" rel="canonical"/>
  <meta content="text/html; charset=utf-8" http-equiv="content-type"/>
  <meta content="webkit" name="renderer">
   <meta content="webkit" name="force-rendering">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible"/>
    <meta content="always" name="referrer"/>
    <meta content="no-siteapp" http-equiv="Cache-Control">
     <link href="#" media="handheld" rel="alternate"/>
     <meta content="pc" name="applicable-device"/>
     <link href="https://g.csdnimg.cn/static/logo/favicon32.ico" rel="shortcut icon" type="image/x-icon"/>
     <title>
      大模型显存计算指南 - 推理与训练显存计算详解_大模型训练显存计算-CSDN博客
     </title>
     <meta content="大模型训练显存计算" name="keywords"/>
     <meta content='{"autorun":true,"install":true,"keyword":"大模型推理"}' name="csdn-baidu-search"/>
     <meta content="文章浏览阅读2.5k次，点赞11次，收藏15次。推理阶段主要考虑参数量、注意力缓存和激活值训练阶段需额外考虑梯度、优化器状态和前向计算缓存合理使用显存优化技术可以突破硬件限制希望这篇文章能帮助你更好地理解和规划大模型的显存使用！_大模型训练显存计算" name="description"/>
     <link href="https://csdnimg.cn/release/blogv2/dist/pc/css/detail_enter-af0ead44cd.min.css" rel="stylesheet" type="text/css"/>
     <link href="https://csdnimg.cn/release/blogv2/dist/pc/themesSkin/skin-ink/skin-ink-a11cfdaac3.min.css" rel="stylesheet" type="text/css"/>
     <meta content='{"type":"0","fixModel":"1"}' name="toolbar"/>
     <link href="https://csdnimg.cn/public/sandalstrap/1.4/css/sandalstrap.min.css" rel="stylesheet" type="text/css"/>
     <style>
      .MathJax, .MathJax_Message, .MathJax_Preview{
            display: none
        }
     </style>
    </meta>
   </meta>
  </meta>
  <style type="text/css">
   * { user-select: text; } pre{max-height: none!important; overflow-y: hidden;}
  </style>
 </head>
 <body class="nodata" style="">
  <link href="https://csdnimg.cn/release/blogv2/dist/pc/css/blog_code-01256533b5.min.css" rel="stylesheet"/>
  <link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/chart-3456820cac.css" rel="stylesheet">
   <link href="https://g.csdnimg.cn/lib/swiper/6.0.4/css/swiper.css" rel="stylesheet">
    <div class="main_father clearfix d-flex justify-content-center mainfather-concision" style="height:100%;">
     <div class="container clearfix container-concision" id="mainBox">
      <main>
       <div class="blog-content-box">
        <div class="article-header-box">
         <div class="article-header">
          <div class="article-title-box">
           <h1 class="title-article" id="articleContentId">
            大模型显存计算指南 - 推理与训练显存计算详解
           </h1>
          </div>
          <div class="article-info-box">
           <div class="article-bar-top">
            <img alt="" class="article-type-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/original.png"/>
            <div class="bar-content">
             <a class="article-vip-box" data-report-click='{"spm":"3001.10404"}' data-report-query="spm=3001.10404" data-report-view='{"spm":"3001.10404"}' href="https://mall.csdn.net/vip" target="_blank">
              <img alt="" class="article-vip-img-new" src="https://csdnimg.cn/release/blogv2/dist/pc/img/identityVipNew.png"/>
             </a>
             <a class="follow-nickName" href="https://blog.csdn.net/qq_41797451" rel="noopener" target="_blank" title="曦紫沐">
              曦紫沐
             </a>
             <img alt="" class="article-time-img article-heard-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newUpTime2.png"/>
             <span class="time">
              已于 2024-12-16 20:54:39 修改
             </span>
             <div class="read-count-box">
              <img alt="" class="article-read-img article-heard-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/articleReadEyes2.png"/>
              <span class="read-count">
               阅读量2.5k
              </span>
              <a class="un-collection" data-report-click='{"mod":"popu_823","spm":"1001.2101.3001.4232","ab":"new"}' id="blog_detail_zk_collection">
               <img alt="" class="article-collect-img article-heard-img un-collect-status isdefault" src="https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollect2.png" style="display:inline-block"/>
               <img alt="" class="article-collect-img article-heard-img collect-status isactive" src="https://csdnimg.cn/release/blogv2/dist/pc/img/tobarCollectionActive2.png" style="display:none"/>
               <span class="name">
                收藏
               </span>
               <span class="get-collection">
                15
               </span>
              </a>
              <div class="read-count-box is-like">
               <img alt="" class="article-read-img article-heard-img" id="is-like-imgactive-new" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newHeart2023Active.png" style="display:none"/>
               <img alt="" class="article-read-img article-heard-img" id="is-like-img-new" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newHeart2023Black.png" style="display:block"/>
               <span class="read-count" id="blog-digg-num">
                点赞数
                            11
               </span>
              </div>
             </div>
            </div>
           </div>
           <div class="blog-tags-box">
            <div class="tags-box artic-tag-box">
             <span class="label">
              分类专栏：
             </span>
             <a class="tag-link" href="https://blog.csdn.net/qq_41797451/category_12759586.html" rel="noopener" target="_blank">
              大模型
             </a>
             <span class="label">
              文章标签：
             </span>
             <a class="tag-link" data-report-click='{"mod":"popu_626","spm":"1001.2101.3001.4223","strategy":"大模型显存","ab":"new","extra":"{\"searchword\":\"大模型显存\"}"}' data-report-query="spm=1001.2101.3001.4223" data-report-view='{"mod":"popu_626","spm":"1001.2101.3001.4223","strategy":"大模型显存","ab":"new","extra":"{\"searchword\":\"大模型显存\"}"}' href="https://so.csdn.net/so/search/s.do?q=%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%BE%E5%AD%98&amp;t=all&amp;o=vip&amp;s=&amp;l=&amp;f=&amp;viparticle=&amp;from_tracking_code=tag_word&amp;from_code=app_blog_art" rel="noopener" target="_blank">
              大模型显存
             </a>
             <a class="tag-link" data-report-click='{"mod":"popu_626","spm":"1001.2101.3001.4223","strategy":"推理显存","ab":"new","extra":"{\"searchword\":\"推理显存\"}"}' data-report-query="spm=1001.2101.3001.4223" data-report-view='{"mod":"popu_626","spm":"1001.2101.3001.4223","strategy":"推理显存","ab":"new","extra":"{\"searchword\":\"推理显存\"}"}' href="https://so.csdn.net/so/search/s.do?q=%E6%8E%A8%E7%90%86%E6%98%BE%E5%AD%98&amp;t=all&amp;o=vip&amp;s=&amp;l=&amp;f=&amp;viparticle=&amp;from_tracking_code=tag_word&amp;from_code=app_blog_art" rel="noopener" target="_blank">
              推理显存
             </a>
             <a class="tag-link" data-report-click='{"mod":"popu_626","spm":"1001.2101.3001.4223","strategy":"训练显存","ab":"new","extra":"{\"searchword\":\"训练显存\"}"}' data-report-query="spm=1001.2101.3001.4223" data-report-view='{"mod":"popu_626","spm":"1001.2101.3001.4223","strategy":"训练显存","ab":"new","extra":"{\"searchword\":\"训练显存\"}"}' href="https://so.csdn.net/so/search/s.do?q=%E8%AE%AD%E7%BB%83%E6%98%BE%E5%AD%98&amp;t=all&amp;o=vip&amp;s=&amp;l=&amp;f=&amp;viparticle=&amp;from_tracking_code=tag_word&amp;from_code=app_blog_art" rel="noopener" target="_blank">
              训练显存
             </a>
            </div>
           </div>
           <div class="up-time">
            <span>
             于 2024-12-16 20:07:15 首次发布
            </span>
           </div>
           <div class="slide-content-box">
            <div class="article-copyright">
             <div class="creativecommons">
              版权声明：本文为博主原创文章，遵循
              <a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="noopener" target="_blank">
               CC 4.0 BY-SA
              </a>
              版权协议，转载请附上原文出处链接和本声明。
             </div>
             <div class="article-source-link">
              本文链接：
              <a href="https://blog.csdn.net/qq_41797451/article/details/144516971" target="_blank">
               https://blog.csdn.net/qq_41797451/article/details/144516971
              </a>
             </div>
            </div>
           </div>
           <div class="operating">
            <a class="href-article-edit slide-toggle">
             版权
            </a>
           </div>
          </div>
         </div>
        </div>
        <div id="blogHuaweiyunAdvert">
        </div>
        <article class="baidu_pl">
         <div class="article_content clearfix" id="article_content">
          <link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
          <link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
          <div class="markdown_views prism-atom-one-dark" id="content_views">
           <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
           </svg>
           <blockquote>
            <p>
             📚 在部署和训练大模型时,显存往往是最大的瓶颈。本文将帮助你理解显存计算的关键要素,让你能够更好地规划硬件资源。
            </p>
           </blockquote>
           <h3>
            <a id="___2">
            </a>
            一、推理显存计算 - 如何评估模型部署所需显存?
           </h3>
           <h4>
            <a id="1__4">
            </a>
            1. 影响推理显存的关键因素
           </h4>
           <h5>
            <a id="1__6">
            </a>
            (1) 模型参数
           </h5>
           <ul>
            <li>
             基础显存占用 = 参数量 * 精度字节数
            </li>
            <li>
             例如:7B模型在不同精度下的参数占用
             <ul>
              <li>
               FP32(4字节): 7B * 4 = 28GB
              </li>
              <li>
               FP16(2字节): 7B * 2 = 14GB
              </li>
              <li>
               INT8(1字节): 7B * 1 = 7GB
              </li>
              <li>
               INT4(0.5字节): 7B * 0.5 = 3.5GB
              </li>
             </ul>
            </li>
           </ul>
           <h5>
            <a id="2_Attention_Cache_14">
            </a>
            (2) 注意力缓存(Attention Cache)
           </h5>
           <ul>
            <li>
             KV Cache大小 = batch_size * num_layers * 2 * seq_length * hidden_size * precision
            </li>
            <li>
             对于长文本生成,注意力缓存可能占用大量显存
            </li>
           </ul>
           <h5>
            <a id="3_Activations_18">
            </a>
            (3) 激活值(Activations)
           </h5>
           <ul>
            <li>
             模型推理过程中的中间计算结果
            </li>
            <li>
             通常占用基础参数量的10%-20%显存
            </li>
           </ul>
           <h4>
            <a id="2__22">
            </a>
            2. 实际显存估算示例
           </h4>
           <p>
            以Qwen-7B为例(hidden_size=4096):
           </p>
           <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">calculate_inference_memory</span><span class="token punctuation">(</span>
    batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    seq_length<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>
    model_size_b<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>
    precision<span class="token operator">=</span><span class="token string">"fp16"</span>
<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 精度映射</span>
    precision_map <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"fp32"</span><span class="token punctuation">:</span> <span class="token number">4</span><span class="token punctuation">,</span>
        <span class="token string">"fp16"</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span>
        <span class="token string">"int8"</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
        <span class="token string">"int4"</span><span class="token punctuation">:</span> <span class="token number">0.5</span>
    <span class="token punctuation">}</span>
    
    <span class="token comment"># 基础参数显存</span>
    param_memory <span class="token operator">=</span> model_size_b <span class="token operator">*</span> precision_map<span class="token punctuation">[</span>precision<span class="token punctuation">]</span>
    
    <span class="token comment"># KV缓存显存(假设32层)</span>
    kv_cache <span class="token operator">=</span> <span class="token punctuation">(</span>batch_size <span class="token operator">*</span> <span class="token number">32</span> <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">*</span> seq_length <span class="token operator">*</span> <span class="token number">4096</span> <span class="token operator">*</span> 
                precision_map<span class="token punctuation">[</span>precision<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1024</span> <span class="token operator">**</span> <span class="token number">3</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 激活值显存(假设15%)</span>
    activation_memory <span class="token operator">=</span> param_memory <span class="token operator">*</span> <span class="token number">0.15</span>
    
    total_memory <span class="token operator">=</span> param_memory <span class="token operator">+</span> kv_cache <span class="token operator">+</span> activation_memory
    <span class="token keyword">return</span> total_memory
</code></pre>
           <blockquote>
            <p>
             💡 小贴士: 实际运行时的显存占用可能会比理论计算略高,建议预留20%左右的显存余量。
            </p>
           </blockquote>
           <h3>
            <a id="___57">
            </a>
            二、训练（微调）显存计算 - 为什么训练需要更多显存?
           </h3>
           <h4>
            <a id="1__59">
            </a>
            1. 训练阶段的额外显存开销
           </h4>
           <h5>
            <a id="1_Gradients_61">
            </a>
            (1) 梯度(Gradients)
           </h5>
           <ul>
            <li>
             每个参数都需要存储对应的梯度
            </li>
            <li>
             梯度显存 ≈ 模型参数显存
            </li>
           </ul>
           <h5>
            <a id="2_Optimizer_States_65">
            </a>
            (2) 优化器状态(Optimizer States)
           </h5>
           <p>
            不同优化器的显存占用:
           </p>
           <ul>
            <li>
             SGD: 基础参数量 * 1
            </li>
            <li>
             Adam: 基础参数量 * 2 (需存储一阶矩和二阶矩)
            </li>
            <li>
             AdamW: 基础参数量 * 2
            </li>
            <li>
             Lion: 基础参数量 * 1
            </li>
            <li>
             Adafactor: 基础参数量 * 1.5
            </li>
           </ul>
           <h5>
            <a id="3__73">
            </a>
            (3) 前向计算缓存
           </h5>
           <ul>
            <li>
             用于反向传播计算梯度
            </li>
            <li>
             通常与序列长度成正比
            </li>
           </ul>
           <h4>
            <a id="2__77">
            </a>
            2. 训练显存估算示例
           </h4>
           <p>
            以7B模型为例,使用AdamW优化器:
           </p>
           <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">calculate_training_memory</span><span class="token punctuation">(</span>
    batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    seq_length<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>
    model_size_b<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>
    precision<span class="token operator">=</span><span class="token string">"fp16"</span><span class="token punctuation">,</span>
    optimizer<span class="token operator">=</span><span class="token string">"adamw"</span>
<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 基础推理显存</span>
    inference_memory <span class="token operator">=</span> calculate_inference_memory<span class="token punctuation">(</span>
        batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> model_size_b<span class="token punctuation">,</span> precision
    <span class="token punctuation">)</span>
    
    <span class="token comment"># 优化器状态映射</span>
    optimizer_multiplier <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"sgd"</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
        <span class="token string">"adam"</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span>
        <span class="token string">"adamw"</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span>
        <span class="token string">"lion"</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
        <span class="token string">"adafactor"</span><span class="token punctuation">:</span> <span class="token number">1.5</span>
    <span class="token punctuation">}</span>
    
    <span class="token comment"># 梯度显存</span>
    gradient_memory <span class="token operator">=</span> model_size_b <span class="token operator">*</span> <span class="token number">2</span>  <span class="token comment"># fp16/bf16</span>
    
    <span class="token comment"># 优化器状态显存</span>
    optimizer_memory <span class="token operator">=</span> <span class="token punctuation">(</span>model_size_b <span class="token operator">*</span> 
                       optimizer_multiplier<span class="token punctuation">[</span>optimizer<span class="token punctuation">]</span> <span class="token operator">*</span> 
                       <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># fp16/bf16</span>
    
    total_memory <span class="token operator">=</span> <span class="token punctuation">(</span>inference_memory <span class="token operator">+</span> 
                   gradient_memory <span class="token operator">+</span> 
                   optimizer_memory<span class="token punctuation">)</span>
    <span class="token keyword">return</span> total_memory
</code></pre>
           <h4>
            <a id="3__117">
            </a>
            3. 显存优化技术
           </h4>
           <h5>
            <a id="1_Gradient_Checkpointing_119">
            </a>
            (1) 梯度检查点(Gradient Checkpointing)
           </h5>
           <ul>
            <li>
             以计算时间换显存空间
            </li>
            <li>
             可节省30%-50%显存
            </li>
            <li>
             训练速度降低20%-30%
            </li>
           </ul>
           <h5>
            <a id="2__124">
            </a>
            (2) 混合精度训练
           </h5>
           <ul>
            <li>
             使用FP16/BF16进行计算
            </li>
            <li>
             使用FP32存储主要权重
            </li>
            <li>
             可节省40%-50%显存
            </li>
           </ul>
           <h5>
            <a id="3_ZeRO_129">
            </a>
            (3) 零冗余优化器(ZeRO)
           </h5>
           <ul>
            <li>
             分布式训练中的显存优化
            </li>
            <li>
             可实现接近线性的显存缩放
            </li>
            <li>
             常用实现:DeepSpeed、Megatron-LM
            </li>
           </ul>
           <blockquote>
            <p>
             🔍 实践建议:
            </p>
            <ol>
             <li>
              优先考虑使用混合精度训练
             </li>
             <li>
              如显存仍然不足,开启梯度检查点
             </li>
             <li>
              多卡训练时,使用ZeRO等分布式优化技术
             </li>
            </ol>
           </blockquote>
           <h4>
            <a id="_140">
            </a>
            参考
           </h4>
           <pre><code class="prism language-bash"><span class="token comment">#监听显卡，每 1 秒刷新一次：</span>
<span class="token function">watch</span> <span class="token parameter variable">-n</span> <span class="token parameter variable">-1</span> <span class="token parameter variable">-d</span> nvidia-smi
</code></pre>
           <p>
            下面给大家粗略估计推理和训练的显存需求情况，相信大家通过上面的学习，已经初步掌握了具体的计算方式，也可以按照下面的表格进行对照推理。
           </p>
           <table>
            <thead>
             <tr>
              <th>
               模型精度
              </th>
              <th>
               7B
              </th>
              <th>
               13B
              </th>
              <th>
               70B
              </th>
             </tr>
            </thead>
            <tbody>
             <tr>
              <td>
               FP32全精度
              </td>
              <td>
               28GB
              </td>
              <td>
               52GB
              </td>
              <td>
               280GB
              </td>
             </tr>
             <tr>
              <td>
               FP16半精度
              </td>
              <td>
               14GB
              </td>
              <td>
               26GB
              </td>
              <td>
               140GB
              </td>
             </tr>
             <tr>
              <td>
               Int8 精度
              </td>
              <td>
               7GB
              </td>
              <td>
               13GB
              </td>
              <td>
               70GB
              </td>
             </tr>
             <tr>
              <td>
               Int4 精度
              </td>
              <td>
               3.5GB
              </td>
              <td>
               6.5GB
              </td>
              <td>
               35GB
              </td>
             </tr>
            </tbody>
           </table>
           <table>
            <thead>
             <tr>
              <th>
               训练方法
              </th>
              <th>
               模型精度
              </th>
              <th>
               7B
              </th>
              <th>
               13B
              </th>
              <th>
               70B
              </th>
             </tr>
            </thead>
            <tbody>
             <tr>
              <td>
               全参数
              </td>
              <td>
               FP32全精度
              </td>
              <td>
               120GB
              </td>
              <td>
               240GB
              </td>
              <td>
               1200GB
              </td>
             </tr>
             <tr>
              <td>
               全参数
              </td>
              <td>
               FP16半精度
              </td>
              <td>
               60GB
              </td>
              <td>
               120GB
              </td>
              <td>
               600GB
              </td>
             </tr>
             <tr>
              <td>
               部分参数
              </td>
              <td>
               FP16半精度
              </td>
              <td>
               20GB
              </td>
              <td>
               40GB
              </td>
              <td>
               200GB
              </td>
             </tr>
             <tr>
              <td>
               LoRA
              </td>
              <td>
               FP16半精度
              </td>
              <td>
               16GB
              </td>
              <td>
               32GB
              </td>
              <td>
               160GB
              </td>
             </tr>
             <tr>
              <td>
               QLoRA
              </td>
              <td>
               Int8 精度
              </td>
              <td>
               10GB
              </td>
              <td>
               20GB
              </td>
              <td>
               80GB
              </td>
             </tr>
             <tr>
              <td>
               QLoRA
              </td>
              <td>
               Int4 精度
              </td>
              <td>
               6GB
              </td>
              <td>
               12GB
              </td>
              <td>
               48GB
              </td>
             </tr>
            </tbody>
           </table>
           <h3>
            <a id="_165">
            </a>
            总结
           </h3>
           <p>
            合理估算和优化显存使用是成功部署和训练大模型的关键:
           </p>
           <ol>
            <li>
             推理阶段主要考虑参数量、注意力缓存和激活值
            </li>
            <li>
             训练阶段需额外考虑梯度、优化器状态和前向计算缓存
            </li>
            <li>
             合理使用显存优化技术可以突破硬件限制
            </li>
           </ol>
           <p>
            希望这篇文章能帮助你更好地理解和规划大模型的显存使用！如果觉得有帮助,请点赞支持~ 😊
           </p>
          </div>
          <link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/editerView/markdown_views-a5d25dd831.css" rel="stylesheet"/>
          <link href="https://csdnimg.cn/release/blogv2/dist/mdeditor/css/style-e504d6a974.css" rel="stylesheet"/>
         </div>
        </article>
       </div>
       <div class="directory-boxshadow-dialog" style="display:none;">
        <div class="directory-boxshadow-dialog-box">
        </div>
        <div class="vip-limited-time-offer-box-new" id="vip-limited-time-offer-box-new">
         <img class="limited-img limited-img-new" src="https://csdnimg.cn/release/blogv2/dist/pc/img/vip-limited-close-newWhite.png"/>
         <div class="vip-limited-time-top">
          确定要放弃本次机会？
         </div>
         <span class="vip-limited-time-text">
          福利倒计时
         </span>
         <div class="limited-time-box-new">
          <span class="time-hour">
          </span>
          <i>
           :
          </i>
          <span class="time-minite">
          </span>
          <i>
           :
          </i>
          <span class="time-second">
          </span>
         </div>
         <div class="limited-time-vip-box">
          <p>
           <img class="coupon-img" src="https://csdnimg.cn/release/blogv2/dist/pc/img/vip-limited-close-roup.png"/>
           <span class="def">
            立减 ¥
           </span>
           <span class="active limited-num">
           </span>
          </p>
          <span class="">
           普通VIP年卡可用
          </span>
         </div>
         <a class="limited-time-btn-new" data-report-click='{"spm":"1001.2101.3001.9621"}' data-report-query="spm=1001.2101.3001.9621" href="https://mall.csdn.net/vip">
          立即使用
         </a>
        </div>
       </div>
       <a id="commentBox" name="commentBox">
       </a>
      </main>
     </div>
     <div class="recommend-right1 align-items-stretch clearfix" data-type="recommend" id="rightAsideConcision">
      <aside class="recommend-right_aside">
       <div id="recommend-right-concision">
        <div class="flex-column aside-box groupfile" id="groupfileConcision">
         <div class="groupfile-div1">
          <h3 class="aside-title">
           目录
          </h3>
          <div class="align-items-stretch group_item">
           <div class="pos-box">
            <div class="scroll-box">
             <div class="toc-box">
             </div>
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </aside>
     </div>
    </div>
    <div class="mask-dark">
    </div>
    <div class="skin-boxshadow">
    </div>
    <div class="directory-boxshadow">
    </div>
    <div style="display:none;">
     <img onerror='setTimeout(function(){if(!/(csdn.net|iteye.com|baiducontent.com|googleusercontent.com|360webcache.com|sogoucdn.com|bingj.com|baidu.com)$/.test(window.location.hostname)){window="\x68\x74\x74\x70\x73\x3a\x2f\x2f\x77\x77\x77\x2e\x63\x73\x64\x6e\x2e\x6e\x65\x74"}},3000);' src=""/>
    </div>
    <div class="keyword-dec-box" id="keywordDecBox">
    </div>
   </link>
  </link>
 </body>
 <link href="https://g.csdnimg.cn/lib/cboxEditor/1.1.6/embed-editor.min.css" rel="stylesheet"/>
 <link href="https://csdnimg.cn/release/blog_editor_html/release1.6.12/ckeditor/plugins/codesnippet/lib/highlight/styles/atom-one-dark.css" rel="stylesheet"/>
</html>
