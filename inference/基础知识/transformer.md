> https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html
# RNN
RNN很难平行化，
# transformer:Attention is all you need
- 利用self-Attention Layer来替代RNN,但可以做到**平行化**，不需要计算前N个内容就可以计算出N+1个内容

## transformation也即Q,K,V
transformation:改观，变化，转变
```
$a^2 + b^2 = c^2$
```