加载预训练的模型和权重。预训练的模型权重指的是什么，在哪里，像sglang这种推理框架加载权重的时候，怎么加载的，这个权重和RNN CNN transformer什么关系。
# 预训练的模型是什么？
1. 模型结构（Model Architecture）
是指神经网络的拓扑结构：比如有多少层、每层是什么类型（全连接、卷积、注意力层）、激活函数等等。
举个例子：
CNN 结构一般会有卷积层、池化层、全连接层；
RNN 有循环结构；
Transformer 有多层 self-attention 和 feedforward 层。这就像是“神经网络的骨架”。
# 模型权重（Model Weights / Parameters）
是指训练好的网络中，每一层的参数（比如权重矩阵和偏置向量）。这些是通过训练数据学出来的，是模型的「大脑」。
所以「预训练的模型权重」就是：
在大量数据上，别人已经帮你训练好的模型参数。

# 权重在哪里？
这些训练好的模型权重一般是保存成**二进制格式**，比如 PyTorch 用 .pt、.bin、.safetensors 等格式，保存成文件。它们通常：
上传在 HuggingFace、OpenAI、Google 的仓库里；
对应模型结构的定义，比如 llama2, chatglm3, bert, gpt2 等。
# 像 sglang 这种推理框架怎么加载这些权重？
以 sglang 为例，它是一种 推理框架（inference framework），常用于加载大模型并高效运行推理流程。
加载步骤一般如下：
1. 加载模型结构
用代码定义模型结构，例如用transformers.AutoModelForCausalLM.from_pretrained(...) 加载一个 GPT 类模型的结构。
2. 加载权重
框架从指定路径（本地或网络）读取 .bin 或 .safetensors 文件；将里面的参数赋值到模型结构中每一层；最终模型 ready to run。sglang 支持加载多个后端（如 HuggingFace Transformers、vLLM、MLC、ggml等），本质上也是调用这些底层模块来加载权重。
# 四、权重和 RNN/CNN/Transformer 的关系
模型结构决定了参数的「形状」；预训练权重是为某个结构特定训练的参数值；
所以：你不能拿一个 RNN 的权重去给 CNN 模型用；Transformer 模型的预训练权重只能配给相同架构的 Transformer 使用；如果你换结构，必须重新训练。

# demo
## 以 LLaMA 模型为例（Transformer 架构）
```
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")  # 加载结构+权重
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
output = model.generate(tokenizer("你好", return_tensors="pt")["input_ids"])
```
from_pretrained(...) 会：
- 自动下载模型结构定义；
- 下载 .bin 或 .safetensors 的权重文件；
- 将权重 load 到结构中。
```
对output = model.generate(tokenizer("你好", return_tensors="pt")["input_ids"])详细讲解
```
- 使用 tokenizer（分词器）把输入字符串 "你好" 转换成模型可以理解的张量（tensor）。
- return_tensors="pt" 表示返回的是 PyTorch 张量（pt 代表 PyTorch，tf 代表 TensorFlow）。
- 返回的是一个字典，比如：
```
{
    "input_ids": tensor([[1234, 5678]]),
    "attention_mask": tensor([[1, 1]])
}
从上面的字典中取出 input_ids 键对应的值，这是模型实际要用来生成的输入。
```
- model.generate(...)
调用模型的 generate() 方法，对输入进行生成。这是 Transformer 模型中用来做文本生成 的方法，比如自动续写、对话回复等。
- output = ...
把生成的结果保存到 output 变量中。这个 output 通常是一个张量，比如：tensor([[1234, 5678, 9999, 8888]])
可以通过 tokenizer.decode(output[0]) 把它解码成中文字符串。
**PS**补充以下部分，让整个代码更加完整
result = tokenizer.decode(output_ids[0], skip_special_tokens=True)
是对模型生成的 token 序列进行解码，把它还原成人类可读的文本。
skip_special_tokens=True
这一点很重要。很多模型使用特殊 token 来表示：
1.开始（比如 <s>）
2.结束（比如 </s>）
3.填充（<pad>）
4.分隔（<sep>）等
5.设置 skip_special_tokens=True 会自动 跳过这些符号，否则输出结果中可能会有 <s>、</s> 这种模型内部用的标记。

# 总结
概念	    含义
模型结构	网络的层级结构，像骨架
模型权重	每一层的训练后参数值，像脑子
权重文件	.pt, .bin, .safetensors，用来保存参数
权重本质	多个 float32/float16 类型的张量
加载过程	框架读取文件 → 建模型结构 → 把值塞进去概念