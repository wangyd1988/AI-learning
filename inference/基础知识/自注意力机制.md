# 是什么
自注意力机制通过元素之间的交互来捕捉长距离依赖关系。在机器翻译中，某个词的含义可能会受到句子中其他词的影响，自注意力机制可以捕捉这种依赖关系。它的计算方式通过查询（Query）、键（Key）、值（Value）这三个向量来进行的，计算注意力分数，然后用softmax归一化，再与Value加权求和，得到新的表示。这样可以让模型动态地关注到重要的部分。
# 自注意力机制与多层感知关系
MLP通过多层非线性变换来学习复杂的模式，而自注意力机制则是通过元素之间的交互来捕捉长距离依赖关系。可能它们都是构建深度学习模型的组件。MLP的作用可能是在自注意力处理后进一步对每个位置的表示进行非线性变换和增强。因此，在Transformer中，自注意力负责捕捉序列内部的依赖关系，而MLP则负责对每个位置的独立处理，增加模型的表达能力。

MLP和自注意力机制都可以作为神经网络中的基本构建块，但适用的场景不同。MLP适合处理非序列数据，或者需要逐层抽象的任务，而自注意力更适合处理序列数据，尤其是长序列，因为它可以处理任意长度的依赖关系，而不像RNN那样有梯度消失的问题。

MLP和自注意力机制都是神经网络中的组件，用于特征转换。自注意力机制通过元素间的交互动态计算权重，适合处理序列数据中的长距离依赖；而MLP通过全连接层和非线性激活逐层处理数据。在Transformer中，两者结合使用，自注意力捕捉依赖，MLP增强每个位置的特征。它们的参数和计算方式不同，但可以互补，结合使用提升模型性能。

#  MLA -- 为成本优化而生的注意力机制 
MLA的本质是将一个矩阵切分成两个矩阵，也就是切分后的两个矩阵相乘能够得到最初的矩阵，例如将形状为5*3的矩阵，分成5*4和4*3的矩阵
![image](https://mmbiz.qpic.cn/sz_mmbiz_png/Hg5PE5UxuiccGpWNE2cHboqTMIcD67s91KMW4Nn0F9ic9PnbrDxONm0WYwr8aBV9clK3MLXskaZEnQBuFp8cUDiaw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1

# KVCache
大模型推理的成本的瓶颈在哪里？答案可能会出乎意料 —— 是显存。显卡有大量计算单元，而推理任务又是线性的一次只能出一个 Token，为了充分利用显卡的计算资源达到最大的吞吐，一般会同时运行尽可能多的生成任务。而每个任务在推理过程中都会占用大量显存，如果想运行尽可能多的任务就需要运行时显存占用足够小。而 MLA 在运行时的显存占用是原始注意力机制的 6.7%，你没看错，不是降低了 6.7%，是降低了 93.3%。打个比喻这一刀下去不是腰斩，而是脚踝斩。在不考虑模型本身的显存占用情况下，近似可以认为 MLA 在相同显存下可以容纳 15 倍的生成任务。

虽然 MLA 的论文里没有细说这个灵感的来源，但我认为他们是从原先的 KVCache 倒推，诞生了一种全新的注意力机制。到这里就不得不说下 KVCache 是什么。

大模型每生成一个 Token 都需要对之前所有的 Token 进行计算，来得出最新的一个 Token 是什么。但是一般任务不会生成一个 Token 就结束，往往要一直生成到结束符号，这就会导致前面的 Token 每一次都要重复计算。于是 KVCache 的作用就是把之前每个 Token 计算生成的中间结果保存下来，这样就可以避免重复计算了。你可以理解为每个 Token 都被映射成了一个 1000 * 1000 的矩阵，那么我们有没有什么办法来减少这个矩阵的内存占用呢？
# MLA
这里有意思的事情终于可以开始了，我们可以用两个小矩阵相乘来近似一个大矩阵。这里刚才你还记得的线性代数知识可以用上了，1000 * 2 的矩阵乘一个 2 * 1000 的矩阵也可以得到一个 1000 * 1000 的矩阵，而这两个矩阵总共只有 4000 个元素，是 1000 * 1000 矩阵元素数量的 0.4%。

这就是 MLA 在数学上最核心的思路了，在 DeepSeek V2 中，本来一个 Token 应该被映射为一个 1*16k 的向量，而在使用 MLA 后会先通过一个压缩矩阵将这个 Token 映射为 1*512 的向量，等到需要的时候再通过一个 512 * 16k 的解压矩阵还原成 1*16k 的向量。在这里压缩矩阵和解压矩阵都是通过训练得来，是模型的一部分只会占用固定的显存，而运行时针对每个 Token 的显存占用就只剩这个 1*512 的向量，只有原来的 3%。

一个完整的对比如下图所示，原先的 MHA 需要 Cache 完整矩阵，而 MLA 只需要 Cache 中间压缩后的一个向量，再还原出完整矩阵。
![image](https://mmbiz.qpic.cn/sz_mmbiz_png/Hg5PE5UxuiccGpWNE2cHboqTMIcD67s91MkfB7sRWibFXCNMNBBRMfTiav8NCSWpqrHK66y5Vv8Aoyg9nYMh07CGw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)