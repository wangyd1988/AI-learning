> https://demo.leemeng.tw/

-
# 卷积神经网路：一个映射函数
卷积神经网路（Convoluational Neural Network, 后简称为CNN）是一种神经网路架构。任何类型的神经网路本质上都是一个映射函数。它们会在内部进行一连串特定的数据转换步骤，想办法将给定的输入数据转换成指定的输出形式。

# 卷积运算：撷取局部特征
卷积运算里头最不可或缺的就是滤波器（filter）的存在了。常见的滤波器大小为3 x 3 像素，下文皆以此为例。有了滤波器以后，我们会将其用来对图片中每个对应的3 x 3 范围做卷积运算,其中图中每次滤波器往右以及往下的距离（stride）为1：![image](https://leemeng.tw/images/cat_dog_classifier/convolution-layer-a.png)

事实上，卷积就是将滤波器里头的每个数字拿去跟图片对应位置的像素值相乘，再把所有相乘结果加起来：![image](https://leemeng.tw/images/cat_dog_classifier/3D_Convolution_Animation.gif)
每个滤波器对输入图片做的卷积运算事实上是一种特征提取（**feature extraction**）步骤。
不同的滤波器会对图中所有相同范围（3 x 3）的像素做不同的转换，进而从这些像素里头提取出：
- [x] 物件轮廓
- [x] 左上突出
- [x] 边缘线条
另外有趣的是，我们可以将第一组滤波器的输出结果当成新的输入「图片」，接着使用另外一组新的滤波器，再次对这些新的输入做卷积。![image](https://leemeng.tw/images/cat_dog_classifier/input-to-output-feature-maps.jpg)
新的运算结果让我们提取出新的特征，并再次成为下一组滤波器的输入。透过重复几次这样的转换，我们相信最终得到的图片特征能够包含原始图片中的重要资讯。
我们可以利用这些特征（features）来做很多事情，比方说丢给输出为一个神经元的全连接层做二元分类（binary classification），进而产生图片里有猫咪或狗狗的机率。事实上，这就是本文的卷积神经网路CNN 在做的事情：
![image](https://leemeng.tw/images/cat_dog_classifier/cnn-architecture-n.jpg)
==一般而言，CNN 会重复多次卷积、池化运算以提取图片特征，接着将萃取出来的图片特征交给全连接层做分类==


**CNN 里头这些滤波器的值从哪来的？前面的滤波器因为我们已经知道它们的值，所以当然可以直接拿来做卷积，但是在CNN 里头，我们不可能手动一个个设定滤波器里头的值吧？**

我们可以随机初始化所有滤波器的数值，并利用平常训练神经网路的反向传播算法（Backpropagation），让CNN 自己学出一组有用的滤波器数值来将输入图片（一大堆像素）转换成我们想要的值（一个猫咪机率）。透过这些转换，CNN 能帮我们萃取出有用的图片特征。而这也是深度学习最厉害一个的地方：自动化特征工程（feature engineering）。
# 池化运算：降低取样
进行池化时，我们一样会有一个滤波器（filter）扫过整张图片，但这个滤波器跟卷积时的滤波器不同的地方有几点：

- [x] 一般来说池化的滤波器大小为2 x 2（卷积滤波器的大小通常为1, 3, 5）
- [x] 滤波器每次往右、往下移动距离（stride）为2（卷积通常为1）
- [x] 滤波器里头本身没有数值，而是将2 x 2 范围里头的最大值取出来（卷积则是透过自己的数值跟输入做相乘计算）
![image](https://leemeng.tw/images/cat_dog_classifier/max-pooling-steps.jpg)
上图2 x 2 的红框即为池化时用的滤波器。而将每个范围内最大数值取出的max pooling 是最常见的做法，但你也可以选择取4 个数字平均的average pooling。
以动画表示池化运算的话就如下方所示：![image](https://leemeng.tw/images/cat_dog_classifier/max-pooling-a.png)

==跟解释卷积运作原理时一样，池化运算本身不难，事实上这就是对图片降低取样（downsampling）：取每个子区块的最大值并降低整体图片像素数量.==

**这样的运算有什么效果呢？**
![image](https://leemeng.tw/images/cat_dog_classifier/downsampling.jpg)
直观来说，一个2 x 2 的池化运算能让我们在不影响物体的情况下，将原图大小缩减到原来的25 %。

对你而言，左边和右边的图片都代表着同一只猫咪。透过池化，我们将图片特征数量缩减到25 % 并保留原先资讯。这样做最明显的好处是能减少神经网路所需处理的像素与计算量，加快模型的训练速度。
# CNN主要做了哪些
- [x] CNN 是一个利用卷积与池化对输入图片做特征撷取的神经网路架构
- [x] 对图片做卷积是有意义的数据转换是因为：
   1. 很多图形pattern 尺寸比原图小很多
   2. 同样pattern 会重复出现在很多地方
- [x] 对图片做池化是有意义的数据转换是因为：
   1. 对像素降低取样并不会改变图中物件
   2. 减少神经网路所需处理的数据量
- [x] 卷积后常跟着池化运算，而你可以重复做（卷积-> 池化）步骤多次来萃取图片特征
- [x] 最后得到的图片特征可以交给CNN 里的全连接层，由它为我们做（猫狗）分类
- [x] CNN 里的卷积、池化扮演着萃取特征的角色，而全连接层则扮演着分类器的角色

**任何类型的神经网路本质上都是一个映射函数。它们会在内部进行一连串特定的数据转换步骤，想办法将给定的输入数据转换成指定的输出形式。**
你没办法简单写一个Python 函式f(x)在一个步骤里头将输入图片转换成猫咪机率，但你可以将一个已训练好的CNN 当作f(x)，并透过它将输入图片的像素x经过一连串的数据转换（卷积、池化、全连接层）逐渐转换成一个猫咪机率y。

每过一层转换，萃取出来的资讯就越来越不像原始像素，而越来越靠近分类结果（猫或狗）。

**深度学习就是一个「资讯提炼」的管道。透过不断的数据转换步骤，将原始数据中不重要的资讯筛去，并把对眼前任务重要的资讯留下来。**



